{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiuBqDB1Uv6PgOm865dEgy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aqibaman/building-llm-applications-from-scratch-course/blob/main/Module_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPpA9EmKdJBL"
      },
      "outputs": [],
      "source": [
        "!pip install nltk --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # This line is added to download the missing resource\n",
        "\n",
        "# Create a placeholder for the language model\n",
        "# Using nested defaultdict to automatically handle new keys\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Build the trigram model\n",
        "for sentence in reuters.sents():\n",
        "    # Generate trigrams from each sentence\n",
        "    # pad_right and pad_left add None at the beginning and end of the sentence\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        # Increment the count for this trigram\n",
        "        model[(w1, w2)][w3] += 1\n",
        "\n",
        "# Convert frequency counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    # Calculate total count for this bigram\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "\n",
        "    # Convert each count to a probability\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n",
        "\n",
        "# At this point, model contains the probabilities of each word (w3)\n",
        "# given the previous two words (w1, w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHnEGU3xdKYb",
        "outputId": "b16ea256-cf68-4065-a9da-a6b59afcbb50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the probabilities for words following \"the price\" and sort them\n",
        "# This creates a list of (word, probability) tuples, sorted by probability in descending order\n",
        "sorted_probabilities = sorted(dict(model[\"the\", \"price\"]).items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sorted probabilities\n",
        "print(\"Most probable words following 'the price', in order:\")\n",
        "for word, prob in sorted_probabilities:\n",
        "    print(f\"{word}: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s4thaekdSLL",
        "outputId": "5a60b780-7363-4211-9440-6b899be11950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most probable words following 'the price', in order:\n",
            "of: 0.3209302325581395\n",
            "it: 0.05581395348837209\n",
            "to: 0.05581395348837209\n",
            "for: 0.05116279069767442\n",
            ".: 0.023255813953488372\n",
            "at: 0.023255813953488372\n",
            "adjustment: 0.023255813953488372\n",
            "is: 0.018604651162790697\n",
            ",: 0.018604651162790697\n",
            "paid: 0.013953488372093023\n",
            "increases: 0.013953488372093023\n",
            "per: 0.013953488372093023\n",
            "the: 0.013953488372093023\n",
            "will: 0.013953488372093023\n",
            "cut: 0.009302325581395349\n",
            "cuts: 0.009302325581395349\n",
            "(: 0.009302325581395349\n",
            "differentials: 0.009302325581395349\n",
            "has: 0.009302325581395349\n",
            "stayed: 0.009302325581395349\n",
            "was: 0.009302325581395349\n",
            "freeze: 0.009302325581395349\n",
            "increase: 0.009302325581395349\n",
            "would: 0.009302325581395349\n",
            "yesterday: 0.004651162790697674\n",
            "effect: 0.004651162790697674\n",
            "used: 0.004651162790697674\n",
            "climate: 0.004651162790697674\n",
            "reductions: 0.004651162790697674\n",
            "limit: 0.004651162790697674\n",
            "now: 0.004651162790697674\n",
            "moved: 0.004651162790697674\n",
            "adjustments: 0.004651162790697674\n",
            "slumped: 0.004651162790697674\n",
            "move: 0.004651162790697674\n",
            "evolution: 0.004651162790697674\n",
            "went: 0.004651162790697674\n",
            "factor: 0.004651162790697674\n",
            "Royal: 0.004651162790697674\n",
            "again: 0.004651162790697674\n",
            "changes: 0.004651162790697674\n",
            "holds: 0.004651162790697674\n",
            "fall: 0.004651162790697674\n",
            "-: 0.004651162790697674\n",
            "from: 0.004651162790697674\n",
            "base: 0.004651162790697674\n",
            "on: 0.004651162790697674\n",
            "review: 0.004651162790697674\n",
            "while: 0.004651162790697674\n",
            "collapse: 0.004651162790697674\n",
            "being: 0.004651162790697674\n",
            "outlook: 0.004651162790697674\n",
            "rises: 0.004651162790697674\n",
            "drop: 0.004651162790697674\n",
            "guaranteed: 0.004651162790697674\n",
            ",\": 0.004651162790697674\n",
            "structure: 0.004651162790697674\n",
            "and: 0.004651162790697674\n",
            "could: 0.004651162790697674\n",
            "related: 0.004651162790697674\n",
            "hike: 0.004651162790697674\n",
            "we: 0.004651162790697674\n",
            "policy: 0.004651162790697674\n",
            "revision: 0.004651162790697674\n",
            "led: 0.004651162790697674\n",
            "action: 0.004651162790697674\n",
            "zone: 0.004651162790697674\n",
            "slump: 0.004651162790697674\n",
            "had: 0.004651162790697674\n",
            "difference: 0.004651162790697674\n",
            "in: 0.004651162790697674\n",
            "raise: 0.004651162790697674\n",
            "support: 0.004651162790697674\n",
            "gap: 0.004651162790697674\n",
            "projected: 0.004651162790697674\n",
            "approached: 0.004651162790697674\n",
            "instability: 0.004651162790697674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHr_s0aedgBK",
        "outputId": "2c25c6d0-dffb-4133-8b5f-004671e3b51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eIrlna-eC6y",
        "outputId": "919dfd99-c46b-4667-ad50-953338141f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1042301/1042301 [00:00<00:00, 2011565.50B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 1293389.92B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input text\n",
        "text = \"I am thinking\"\n",
        "print(f\"Input text: {text}\")\n",
        "\n",
        "# Encode the input text\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "# Convert indexed tokens to a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# Check if CUDA is available and move model and tensors to GPU if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokens_tensor = tokens_tensor.to(device)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL0yTIN9e4CE",
        "outputId": "89154081-74c8-4e66-c934-17bff9ffec36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: I am thinking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 665/665 [00:00<00:00, 417359.29B/s]\n",
            "100%|██████████| 548118077/548118077 [00:15<00:00, 36079395.39B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict next token\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# Get the predicted next sub-word (token)\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_token = tokenizer.decode([predicted_index])\n",
        "\n",
        "# Add the predicted token to the original text\n",
        "predicted_text = text + predicted_token\n",
        "\n",
        "# Print the results\n",
        "print(f\"Predicted next token: '{predicted_token}'\")\n",
        "print(f\"Complete predicted text: '{predicted_text}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qFp6Zl3e8dK",
        "outputId": "6cf0b1b9-ce18-43ac-8c9d-6eddc34e5916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next token: ' of'\n",
            "Complete predicted text: 'I am thinking of'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Generate multiple next tokens\n",
        "num_tokens_to_generate = 10\n",
        "generated_text = text\n",
        "\n",
        "for _ in range(num_tokens_to_generate):\n",
        "    # Encode all text generated so far\n",
        "    indexed_tokens = tokenizer.encode(generated_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "\n",
        "    # Predict next token\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "    # Get the predicted next token\n",
        "    predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "    predicted_token = tokenizer.decode([predicted_index])\n",
        "\n",
        "    # Add the predicted token to the generated text\n",
        "    generated_text += predicted_token\n",
        "\n",
        "print(f\"\\nGenerated text with {num_tokens_to_generate} additional tokens:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iye8WefHfHuD",
        "outputId": "1304cbf7-59cd-4a24-a47b-fa2bacb9b13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text with 10 additional tokens:\n",
            "I am thinking of doing a book about the history of the United\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qHVv7xX0fMPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}